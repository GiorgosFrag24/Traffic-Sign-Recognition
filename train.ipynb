{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7942ZljIaCF","executionInfo":{"status":"ok","timestamp":1635005075441,"user_tz":-180,"elapsed":41302,"user":{"displayName":"Giorgos Fragk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKkVwY9WJs2Lug2HGJl4Y9km0riZgZCL7xlkMeaA=s64","userId":"05826907546390853125"}},"outputId":"8979599c-e0b6-4401-a6e9-7c53b1c9503d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Ktps-OhVI035"},"source":["# Placeholders for training data\n","data = []\n","labels = []\n","classes = 43\n","cur_path = r'/content/drive/MyDrive/Traffic Sign Recognition'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcNRpBmoIvfN"},"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import cv2\n","import tensorflow as tf\n","from PIL import Image\n","from PIL import ImageFilter\n","import os\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-66pl21f8grb"},"source":["\n","# Define a Gaussian noise function\n","def noise(image):\n","    row,col,ch= image.shape      \n","    gauss = np.array(image.shape)\n","    gauss = np.random.standard_normal(size=(row,col,ch))\n","    noisy = image + 0.1*255*gauss\n","    return noisy\n","    \n","\n","# Model creator function\n","def create_model(neurons = 256, dropout = 0.1, filters=16, kernels=[(5,5),(3,3)],optimizer='adam'):\n","    model = Sequential()\n","    model.add(Conv2D(filters=filters, kernel_size=kernels[0], \n","                    activation='relu', input_shape=(30,30,3) )) \n","    model.add(MaxPool2D(pool_size=(2,2)))\n","    model.add(Dropout(rate=dropout))\n","    model.add(Conv2D(filters=filters*2, kernel_size=kernels[1], activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2,2)))\n","    model.add(Dropout(rate=dropout))\n","    model.add(Flatten())\n","    model.add(Dense(neurons, activation='relu'))\n","    model.add(Dropout(rate=dropout))\n","    model.add(Dense(43, activation='softmax'))\n","\n","    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYyDwFdzI5IH"},"source":["# DON'T EXECUTE!!!\n","# Load Training images and their respective labels\n","for i in range(0,43):\n","    path = os.path.join(cur_path,'GTSRB/Training',str(i).zfill(5))\n","    images = os.listdir(path)    \n","    for a in images:\n","        try:\n","            if a.endswith('.csv'):\n","              continue\n","            image = Image.open(path + '/'+ a)\n","            image = image.resize((30,30))\n","            image = np.array(image)\n","            data.append(image)\n","            labels.append(i)\n","        except Exception as inst:\n","            print(inst)\n","            continue\n","\n","# Store training data \n","data = np.array(data)\n","labels = np.array(labels)\n","#np.save( os.path.join(cur_path,'data.npy'), data)\n","#np.save(os.path.join(cur_path,'labels.npy'), labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YEYjLTGbGEw"},"source":["# Load training data from disk\n","data = np.load(os.path.join(cur_path,'data.npy'))\n","labels = np.load(os.path.join(cur_path,'labels.npy'))\n","\n","# Split training data to train and validation\n","X = data\n","y = labels\n","X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n","\n","# Convert labels to one-hot encoding\n","y_train = to_categorical(y_train, 43)\n","y_val = to_categorical(y_val, 43)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Si8pOfAqkF2-"},"source":["# This block is used to evaluate our models' hyperparameters by iteratively searching the input space\n","# and finding the optimal values through the learning curves\n","model = create_model()\n","model.summary()\n","epochs = 40\n","history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_data=(X_val, y_val))\n","model.save(os.path.join(cur_path,'keras_model.h5'))\n","iteration = 7\n","#Plotting accuracy graphs \n","plt.figure(0)\n","plt.plot(history.history['accuracy'], label='training accuracy')\n","plt.plot(history.history['val_accuracy'], label='val accuracy')\n","plt.title('Accuracy')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","#plt.savefig(os.path.join(r'/content/drive/MyDrive/Traffic Sign Recognition/GTSRB/Plots',str(iteration)+' Set Accuracy .png'))\n","plt.show()\n","\n","#Plotting loss graphs\n","plt.figure(1)\n","plt.plot(history.history['loss'], label='training loss')\n","plt.plot(history.history['val_loss'], label='val loss')\n","plt.title('Loss')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend()\n","#plt.savefig(os.path.join(r'/content/drive/MyDrive/Traffic Sign Recognition/GTSRB/Plots',str(iteration)+' Set Loss .png'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39A-81eu8msQ"},"source":["# DON'T EXECUTE!!!\n","# Input the testing data\n","test = pd.read_csv(os.path.join(cur_path,'GTSRB/Testing/GT-final_test.csv'),delimiter=';')\n","test_imgs = test[\"Filename\"].values\n","test_data=[]\n","for img in test_imgs:\n","    print(counter)\n","    image = Image.open(os.path.join(cur_path,'GTSRB/Testing',img))\n","    image = image.resize((30,30))\n","    test_data.append(np.array(image))\n","X_test = np.array(test_data)\n","y_test = test['ClassId'].values\n","np.save( os.path.join(cur_path,'test_data.npy'), X_test)\n","np.save(os.path.join(cur_path,'test_labels.npy'), y_test) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyuLpLmHJw4N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633915027337,"user_tz":-180,"elapsed":3204,"user":{"displayName":"Polek Espidi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFa388MZZqYiO9TmGaj4DPvGsYtqDbZcIez_19=s64","userId":"05826907546390853125"}},"outputId":"0c28d4ee-f5eb-4573-e9ca-9dd0198f26a0"},"source":["# Load testing data\n","X_test = np.load(os.path.join(cur_path,'test_data.npy'))\n","y_test = np.load(os.path.join(cur_path,'test_labels.npy'))\n","# Make prediction on the test dataset\n","y_pred = model.predict(X_test)\n","y_pred = np.argmax(y_pred, axis=1)\n","y_pred = to_categorical(y_pred, 43) \n","\n","print(\"Final accuracy score is %.5f percent \" %(accuracy_score(y_test, y_pred)*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final accuracy score is 91.83690 percent \n"]}]},{"cell_type":"code","metadata":{"id":"XUNEZNkEyWQS"},"source":["# DON'T EXECUTE!!!\n","# Part 2 of the assignment\n","# Reload the testing images, and distort them by applying a gaussian noise function \n","from matplotlib import pyplot as plt\n","test = pd.read_csv(os.path.join(cur_path,'GTSRB/Testing/GT-final_test.csv'),delimiter=';')\n","y_test = test['ClassId'].values\n","test_imgs = test[\"Filename\"].values\n","noisy_test_data=[]\n","counter=0\n","for img in test_imgs:\n","    image = Image.open(os.path.join(cur_path,'GTSRB/Testing',img))\n","    image = noise(np.array(image)).astype(np.uint8)\n","    image = Image.fromarray(image, 'RGB')\n","    image = image.resize((30,30))\n","    noisy_test_data.append(np.array(image))\n","noisy_X_test=np.array(noisy_test_data)\n","\n","# Save noisy images\n","np.save( os.path.join(cur_path,'noisy_test_data.npy'), noisy_X_test)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcetz2v88VFl","executionInfo":{"status":"ok","timestamp":1633915039156,"user_tz":-180,"elapsed":3166,"user":{"displayName":"Polek Espidi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFa388MZZqYiO9TmGaj4DPvGsYtqDbZcIez_19=s64","userId":"05826907546390853125"}},"outputId":"852aeda4-5819-4201-f008-bd4130465f40"},"source":["# Make a prediction on the distorted images\n","noisy_X_test = np.load(os.path.join(cur_path,'noisy_test_data.npy'))\n","y_pred = model.predict(noisy_X_test)\n","y_pred = np.argmax(y_pred, axis=1)\n","y_pred = to_categorical(y_pred, 43) \n","print(\"Final accuracy score of noisy dataset is %.5f percent \" %(accuracy_score(y_test, y_pred)*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final accuracy score of noisy dataset is 48.93112 percent \n"]}]},{"cell_type":"code","metadata":{"id":"O1L-Ygiune2k"},"source":["import skimage.io\n","import skimage.filters\n","from skimage.filters import unsharp_mask\n","# Define a denoising function\n","def denoise(image):\n","    blurred = skimage.filters.gaussian(\n","    image, sigma=(1, 1), multichannel=True) \n","    sharped = unsharp_mask(blurred, radius=4, amount=4)\n","    return sharped\n","    #return cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n","# http://www.ipol.im/pub/art/2011/bcm_nlm/\n","\n","test = pd.read_csv(os.path.join(cur_path,'GTSRB/Testing/GT-final_test.csv'),delimiter=';')\n","y_test = test['ClassId'].values\n","y_test = to_categorical(y_test, 43) \n","test_imgs = test[\"Filename\"].values\n","denoised_test_data=[]\n","counter=0\n","for img in test_imgs:\n","    image = Image.open(os.path.join(cur_path,'GTSRB/Testing',img))\n","    noisy_image = noise(np.array(image))\n","    \n","    noisy_image = noisy_image.astype(np.uint8) \n","    \n","    cleaned_image = denoise(noisy_image)\n","    plt.imshow(cleaned_image)\n","    break\n","    cleaned_image = cleaned_image.astype(np.uint8)\n","    \n","    image = Image.fromarray(cleaned_image, 'RGB')\n","    image = image.resize((30,30))\n","    denoised_test_data.append(np.array(image))\n","denoised_X_test=np.array(denoised_test_data)\n","\n","# Save recovered images\n","np.save( os.path.join(cur_path,'denoised_test_data.npy'), denoised_X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49T8-qEbu7CY","executionInfo":{"status":"ok","timestamp":1633916531385,"user_tz":-180,"elapsed":3353,"user":{"displayName":"Polek Espidi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFa388MZZqYiO9TmGaj4DPvGsYtqDbZcIez_19=s64","userId":"05826907546390853125"}},"outputId":"6a719e32-2374-46a8-fb74-61a2743dd3f0"},"source":["# Make a prediction on the recovered images\n","denoised_X_test = np.load(os.path.join(cur_path,'denoised_test_data.npy'))\n","y_pred = model.predict(denoised_X_test)\n","y_pred = np.argmax(y_pred, axis=1)\n","y_pred = to_categorical(y_pred, 43) \n","print(\"Final accuracy score of denoised dataset is %.5f percent \" %(accuracy_score(y_test, y_pred)*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final accuracy score of denoised dataset is 4.65558 percent \n"]}]},{"cell_type":"code","metadata":{"id":"TQbVlpKfnfPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633915549638,"user_tz":-180,"elapsed":378,"user":{"displayName":"Polek Espidi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFa388MZZqYiO9TmGaj4DPvGsYtqDbZcIez_19=s64","userId":"05826907546390853125"}},"outputId":"50b2878b-2c35-4d72-b23e-5f039190f96b"},"source":["# Part 3 of the assignment\n","from keras.models import load_model\n","model1 = load_model(os.path.join(cur_path,'keras_model.h5'))\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","\tdef __init__(self):\n","\t\tsuper(Net, self).__init__()\n","\t\t\n","\t\tself.conv1 = nn.Conv2d(3, 16, 5)\n","\t\tself.conv2 = nn.Conv2d(16, 32, 3)\n","\t\tself.fc1 = nn.Linear(800, 256)\n","\t\tself.fc2 = nn.Linear(256, 43)\n","\n","\tdef forward(self, x):\n","\t\tx = F.relu(self.conv1(x))\n","\t\tx = F.max_pool2d(x,2)\n","\t\tx = F.dropout(x)\n","\t\tx = F.relu(self.conv2(x))\n","\t\tx = F.max_pool2d(x,2)\n","\t\tx = F.dropout(x)\n","\t\tx = torch.flatten(x, 1) # flatten all dimensions except batch\n","\t\tx = F.relu(self.fc1(x))\n","\t\tx = F.dropout(x)\n","\t\tx = F.Softmax(self.fc2(x))\n","\t\treturn x\n","\n","net = Net()\n","print(net)\n","\n","# Get the pre-trained weights\n","weights = model1.get_weights()\n","net.conv1.weight.data=torch.from_numpy(np.transpose(weights[0]))\n","net.conv1.bias.data=torch.from_numpy(np.transpose(weights[1]))\n","net.conv2.weight.data=torch.from_numpy(np.transpose(weights[2]))\n","net.conv2.bias.data=torch.from_numpy(np.transpose(weights[3]))\n","\n","net.fc1.weight.data=torch.from_numpy(np.transpose(weights[4]))\n","net.fc1.bias.data=torch.from_numpy(np.transpose(weights[4]))\n","net.fc2.weight.data=torch.from_numpy(np.transpose(weights[6]))\n","net.fc2.bias.data=torch.from_numpy(np.transpose(weights[7]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=800, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=43, bias=True)\n",")\n"]}]}]}